{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 MAGnet Gather Modern Paintings",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhpham2601/Hashtag_GundGallery/blob/main/1_MAGnet_Gather_Modern_Paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZMf_VvYUDpM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMMlsgIB7jfY"
      },
      "source": [
        "file_path = \"/content/gdrive/MyDrive/modern_paintings\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD_7h5mc01N0"
      },
      "source": [
        "!mkdir $file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-0JJXsS6PWY"
      },
      "source": [
        "import urllib #collects several modules for working with URLs\n",
        "import re #regular expression matching operations similar\n",
        "from bs4 import BeautifulSoup #pulling data out of HTML and XML files, works with parser\n",
        "#to provide idiomatic ways of navigating, searching, and modifying the parse tree\n",
        "import time #provides various time-related functions\n",
        "\n",
        "def get_images(url):\n",
        "  print(url)\n",
        "  genre_soup = BeautifulSoup(urllib.request.urlopen(url, \"lxml\")) # 1. represents \n",
        "  #the document as a nested data structure; \"lxml\" - parser\n",
        "  #2. defines functions and classes which help in opening URLs (mostly HTTP) \n",
        "  #in a complex world — basic and digest authentication, redirections, cookies and more\n",
        "  artist_list_main = genre_soup.find(\"main\")\n",
        "  lis = artist_list_main.find_all(\"li\")\n",
        "  # The script goes through each artist on the site alphabetically. It checks to \n",
        "  # see if the artist is tagged as part of the “modern” art movement and if they \n",
        "  # were born after 1800 and died before 1950.\n",
        "\n",
        "  # for each list element\n",
        "   # for each list element\n",
        "  for li in lis: \n",
        "    born = 0\n",
        "    died = 0\n",
        "\n",
        "    # get the date range\n",
        "    for line in li.text.splitlines():\n",
        "      if line.startswith(\",\") and \"-\" in line:\n",
        "        parts = line.split('-')\n",
        "        if len(parts) == 2:\n",
        "          born = int(re.sub(\"[^0-9]\", \"\",parts[0]))\n",
        "          died = int(re.sub(\"[^0-9]\", \"\",parts[1]))\n",
        "\n",
        "    # look for artists who may have created work that could in public domain\n",
        "    if born>1800 and died>0 and died<1950:\n",
        "      link = li.find(\"a\")\n",
        "      artist = link.attrs[\"href\"]\n",
        "\n",
        "      # get the artist's main page\n",
        "      artist_url = base_url + artist\n",
        "      artist_soup = BeautifulSoup(urllib.request.urlopen(artist_url), \"lxml\")\n",
        "\n",
        "      # only look for artists with the word modern on their main page\n",
        "      if \"modern\" in artist_soup.text.lower():\n",
        "        print(artist + \" \" + str(born) + \" - \" + str(died))\n",
        "\n",
        "        # get the artist's web page for the artwork\n",
        "        url = base_url + artist + '/all-works/text-list'\n",
        "        artist_work_soup = BeautifulSoup(urllib.request.urlopen(url), \"lxml\")\n",
        "\n",
        "        # get the main section\n",
        "        artist_main = artist_work_soup.find(\"main\")\n",
        "        image_count = 0\n",
        "        artist_name = artist.split(\"/\")[2]\n",
        "\n",
        "        # get the list of artwork\n",
        "        lis = artist_main.find_all(\"li\")\n",
        "\n",
        "        # for each list element\n",
        "        for li in lis:\n",
        "          link = li.find(\"a\")\n",
        "\n",
        "          if link != None:\n",
        "            painting = link.attrs[\"href\"]\n",
        "\n",
        "            # get the painting\n",
        "            url = base_url + painting\n",
        "            print(url)\n",
        "\n",
        "            try:\n",
        "              painting_soup = BeautifulSoup(urllib.request.urlopen(url), \"lxml\")\n",
        "\n",
        "            except:\n",
        "              print(\"error retreiving page\")\n",
        "              continue\n",
        "\n",
        "            # check the copyright\n",
        "            if \"Public domain\" in painting_soup.text:\n",
        "\n",
        "              # get the url\n",
        "              og_image = painting_soup.find(\"meta\", {\"property\":\"og:image\"})\n",
        "              image_url = og_image[\"content\"].split(\"!\")[0] # ignore the !Large.jpg at the end\n",
        "              print(image_url)\n",
        "\n",
        "              parts = url.split(\"/\")\n",
        "              painting_name = parts[-1]\n",
        "              save_path = file_path + \"/\" + artist_name + \"_\" + painting_name + \".jpg\"\n",
        "\n",
        "              #download the file\n",
        "              try:\n",
        "                print(\"downloading to \" + save_path)\n",
        "                time.sleep(0.2)  # try not to get a 403                    \n",
        "                urllib.request.urlretrieve(image_url, save_path)\n",
        "                image_count = image_count + 1\n",
        "              except Exception as e:\n",
        "                print(\"failed downloading \" + image_url, e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoXxeuZjccOa"
      },
      "source": [
        "base_url = \"https://www.wikiart.org\"\n",
        "urls = []\n",
        "GG_terms = ['abstract-art', 'surrealism', 'abstract-expressionism', \n",
        "             'post-painterly-abstraction', 'pop-art',\n",
        "            'minimalism','post-minimalism','photorealism',\n",
        "           'contemproray','conceptual-art','contemporary-realism',\n",
        "            'hyper-mannerism-anachronism','documentary-photography',\n",
        "          'street-photography']\n",
        "\n",
        "# 'color-field-painting', 'hard-edge-painting',\n",
        "#for c in range(ord('a'), ord('z') + 1)#\n",
        "for c in GG_terms:\n",
        "  #char = chr(c)\n",
        "  artist_list_url = base_url + \"/en/artists-by-art-movement/\" + c + \"#!#resultType:text\"\n",
        "  urls.append(artist_list_url)\n",
        "\n",
        "print(urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDZla14w7wL3"
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "executor = None\n",
        "with ThreadPoolExecutor(max_workers = 8) as executor:\n",
        "  ex = executor\n",
        "  executor.map(get_images, urls)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}